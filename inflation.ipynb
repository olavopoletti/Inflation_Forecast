{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inflation Forecast\n",
    "Evidence suggests that the processes producing inflation change over time. The data relevant to determine the inflation rate during recessions may not be influential during growth periods. That is, the model might change with Business Cycles. It is not straightforward to identify at which point of a cycle the economy is currently on, which hinders the performance of structural approaches. And also increases the time necessary to train data-driven multivariate models. Thus, an univariate, nonlinear model seems to be a good starting point.\n",
    "\n",
    "Long Short-Term Memory (LSTM) is a Recurrent Neural Network (RNN) designed to learn long sequences o data. It does not rely on pre-specified structures or windows - a good fit for forecasting time series.\n",
    "The project is [here](https://github.com/olavopoletti/Inflation_Forecast.git).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the libraries we are going to need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, save_model, load_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a basic layout for our graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic layout for the graphics\n",
    "layout = go.Layout(\n",
    "    font_family=\"Segoe UI\",\n",
    "    font_color=\"Black\",\n",
    "    font_size=14,\n",
    "    title_font_family=\"Segoe UI\",\n",
    "    title_font_color=\"#A79AFF\",\n",
    "    title_font_size=18,\n",
    "    showlegend=False,\n",
    "    margin=dict(b = 10,l= 20,r=10,t=50),\n",
    "    xaxis=dict(\n",
    "        showgrid=False,\n",
    "        gridcolor='DarkGrey',\n",
    "        zeroline=False,\n",
    "        zerolinecolor='White',\n",
    "        zerolinewidth=2,\n",
    "        showticklabels=True,\n",
    "        ),\n",
    "    yaxis=dict(\n",
    "        showgrid=True,\n",
    "        gridcolor='White',\n",
    "        zeroline=True,\n",
    "        zerolinecolor='White',\n",
    "        zerolinewidth=2,\n",
    "        showticklabels=True\n",
    "        ),\n",
    "    plot_bgcolor='#E3DDD5',\n",
    "    paper_bgcolor='#FEFBEA',\n",
    "    autosize=True\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "We are using monthly percentage variations on the Broad National Consumer Price Index available (IPCA) at [BACEN](https://www3.bcb.gov.br/sgspub/localizarseries/localizarSeries.do?method=prepararTelaLocalizarSeries). Let's create a Pandas data frame with data; it will make it easier for us to access the data in the format we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Frame\n",
    "df = pd.read_csv(r\"monthlyInflation.csv\", sep=\";\")\n",
    "df = df.rename(\n",
    "        columns={\n",
    "                \"433 - Broad National Consumer\" \\\n",
    "                + \" Price Index (IPCA) - Monthly % var.\": \"IPCA\"\n",
    "                }\n",
    "        )\n",
    "\n",
    "# Formatting\n",
    "df = df[df.Date != \"Source\"]\n",
    "df[\"Date\"] = df.Date.astype(\"datetime64[ns]\")\n",
    "df[\"IPCA\" ] = df[\"IPCA\" ].astype(\"float\")\n",
    "df = df.set_index('Date')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decomposing the series into its components - Trend, Season, and Residual - gives a better understanding of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decomposing the time series\n",
    "decomposition = seasonal_decompose(df, model='additive')\n",
    "decomposition.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a sharp contrast between the values before and after the 'Real Plan.' Let's check how the series behave before and after July 1994 separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decomposition prior to July 1994\n",
    "decomposition_b = seasonal_decompose(\n",
    "    df[df.index < '1994-07-01'],\n",
    "    model='additive'\n",
    "    )\n",
    "decomposition_b.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decomposition for data after the 'Real Plan'\n",
    "decomposition_a = seasonal_decompose(\n",
    "    df[df.index >= '1994-07-01'],\n",
    "    model='additive'\n",
    "    )\n",
    "decomposition_a.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see changes in the trend and the cycles - besides the magnitude of the value. It suggests that the new plan produced fundamental changes in inflation behavior. Therefore, considering data before the stabilization program would create a model with characteristics that no longer reflect the current structure of the Brazilian economy. Since economic policy takes some time to be effective, it is worthwhile considering some buffer periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataset to exclude data prior to the 'Real Plan'\n",
    "df = df[df.index >= '1995-01-01']\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timeseries analysis depends on stationarity, so let's check for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for Stationarity using Augmented Dickey–Fuller test\n",
    "def adf_test(data):\n",
    "    result = adfuller(data.values)\n",
    "    print('ADF Statistic: %f' % result[0])\n",
    "    print('p-value: %f' % result[1])\n",
    "    for key, value in result[4].items():\n",
    "        print('\\t%s: %.3f' % (key, value))\n",
    "        if result[1] > 0.01:\n",
    "            print(\"Series is not stationary\")\n",
    "        else:\n",
    "            print(\"Series is stationary\")\n",
    "\n",
    "adf_test(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing that the series is stationary, we need to relate each value to its predecessors - we are trying to find how past inflation affects current and future inflation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding lags to the data. We are considering up to 60 periods prior to the target\n",
    "col_names = [\"T-{}\".format(i) for i in range(1, 61)] + [\"IPCA\"]\n",
    "def time_steps(data, lag):\n",
    "\tdf_temp = data\n",
    "\tcolumns = [df_temp.shift(i) for i in range(1, lag+1)]\n",
    "\tcolumns.append(df_temp)\n",
    "\tdf_temp = pd.concat(columns, axis=1)\n",
    "\tdf_temp.fillna(0, inplace=True)\n",
    "\tdf_temp = df_temp.set_axis(col_names, axis=1, inplace=False)\n",
    "\treturn df_temp\n",
    "\n",
    "df_lags = time_steps(df, 60)\n",
    "df_lags.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can separate the data into two sets: one for training the model and the other for testing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test data frames - with the last three years as test\n",
    "train = df_lags[df_lags.index <= '2019-08-01'].values\n",
    "test = df_lags[df_lags.index > '2019-08-01'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data do match the possible outputs of the activation function.\n",
    "\"\"\"\n",
    "Since we are using a Hyperbolic Tangent, the data will be scaled to be \n",
    "between -1 and 1. The scale will be calculated on the training dataset to\n",
    "avoiding biasing de model\n",
    "\"\"\"\n",
    "\n",
    "# Set up the scaler\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler = scaler.fit(train.reshape(-1,1))\n",
    "\n",
    "# Adjust the shape of the data\n",
    "train = train.reshape(train.shape[0], train.shape[1])\n",
    "test = test.reshape(test.shape[0], test.shape[1])\n",
    "\n",
    "# Transform the data\n",
    "train_scaled = scaler.transform(train.reshape(-1,1))\n",
    "test_scaled = scaler.transform(test.reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaled = np.reshape(\n",
    "    train_scaled,\n",
    "    (train.shape[0], train.shape[1], -1)\n",
    "    )\n",
    "test_scaled = np.reshape(\n",
    "    test_scaled,\n",
    "    (test.shape[0], test.shape[1], -1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictors and target for the training set\n",
    "train_x, train_y = train_scaled[:, 0:-1], train_scaled[:, -1]\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After pre-processing, we are ready to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The base model\n",
    "model = Sequential()\n",
    "model.compile(\n",
    "      optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "      loss='mean_squared_error',\n",
    "      metrics=['mean_absolute_error'],\n",
    "      #run_eagerly=True\n",
    "      )\n",
    "\n",
    "# Number of patters to be processed before updating weights\n",
    "batch_size = 1\n",
    "\n",
    "# The LSTM input layer\n",
    "model.add(\n",
    "      LSTM(\n",
    "            360,\n",
    "            input_shape=(60, 1),\n",
    "            #return_sequences=True,\n",
    "            )\n",
    "      )\n",
    "\n",
    "# Hidden LSTM layers \n",
    "#for i in range(0, 60):\n",
    "#      model.add(LSTM(120,return_sequences=True))\n",
    "#model.add(LSTM(120))\n",
    "\n",
    "# The output layer\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "fit = model.fit(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    validation_data=(train_x, train_y),\n",
    "    epochs=720, \n",
    "    batch_size=batch_size,\n",
    "    verbose=0,\n",
    "    shuffle=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for overfitting\n",
    "plt.plot(fit.history['loss'], label='training', color='Blue')\n",
    "plt.plot(fit.history['val_loss'], label='validation',color='Red')\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictors and target for the testing set\n",
    "test_x, test_y = test_scaled[:, 0:-1], test_scaled[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build up state for forecasting\n",
    "train_pred = model.predict(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walk-forward validation - one-step window \n",
    "predictions = list()\n",
    "for i in range(0, test_scaled.shape[0]):\n",
    "\ty_hat = model.predict(test_x[i].reshape((1,60,1)))\n",
    "\tpredictions.extend(y_hat[0])\n",
    "\n",
    "test_pred = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the predictions back to the original scale\n",
    "y_test_hat = scaler.inverse_transform(np.array(predictions).reshape(-1,1))\n",
    "y_test = scaler.inverse_transform(test_y.reshape(-1,1))\n",
    "y_train_hat = scaler.inverse_transform(train_pred.reshape(-1,1))\n",
    "y_train = scaler.inverse_transform(train_y.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance as measured by the Root Mean Squared Error\n",
    "train_score = mean_squared_error(y_train, y_train_hat[:,0])\n",
    "print('Train Score: %.2f MSE' % (train_score))\n",
    "test_score = mean_squared_error(y_test, y_test_hat[:,0])\n",
    "print('Test Score: %.2f MSE' % (test_score))\n",
    "\n",
    "model_error = y_test - y_test_hat[:,0]\n",
    "print('Mean Model Error: ', model_error.mean())\n",
    "\n",
    "trace_1 = go.Scatter(\n",
    "    x=df_lags[df_lags.index>'2019-08-01'].index,\n",
    "    y=df_lags[df_lags.index>'2019-08-01']['IPCA'].values,\n",
    "    name='Actual',\n",
    "    marker=dict(color='SteelBlue')\n",
    "    )\n",
    "\n",
    "trace_2 = go.Scatter(\n",
    "    x=df_lags[df_lags.index>'2019-08-01'].index,\n",
    "    y=y_test_hat.reshape(y_test_hat.shape[0]),\n",
    "    name='Predicted',\n",
    "    marker=dict(color='Firebrick')\n",
    "    )\n",
    "\n",
    "fig_1 = make_subplots(specs=[[{\"secondary_y\": False}]])\n",
    "fig_1.add_trace(trace_1)\n",
    "fig_1.add_trace(trace_2,secondary_y=False)\n",
    "fig_1['layout'].update(\n",
    "    height=600,\n",
    "    width=800,\n",
    "    title='Actual vs Predicted',\n",
    "    xaxis=dict(tickangle=-90),\n",
    "    showlegend=True,\n",
    "    )\n",
    "\n",
    "iplot(fig_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "file_path = Path(r'/home/olavoops/Inflation_Forecast_with_LSTM/model_600Epochs_360Neurons')\n",
    "if file_path.exists():\n",
    "    model_path = input('./?')\n",
    "else:\n",
    "    model_path = file_path\n",
    "\n",
    "#save_model(model, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model_path):\n",
    "    model = Sequential()\n",
    "    hub_layer = KerasLayer(\n",
    "        model_path, \n",
    "        input_shape=[], \n",
    "        dtype=tf.string, \n",
    "        trainable=False\n",
    "        )\n",
    "    model.add(hub_layer)\n",
    "    model.save()\n",
    "\n",
    "save_model(model_path)\n",
    "model = load_model('/content/saved')\n",
    "model.summary()\n",
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = Path('./model_ZeroHidden_1000Epochs_120Neurons')\n",
    "model = load_model(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model_path):\n",
    "    model = tf.keras.Sequential()\n",
    "    hub_layer = hub.KerasLayer(\n",
    "        model_path, \n",
    "        input_shape=[], \n",
    "        dtype=tf.string, \n",
    "        trainable=False\n",
    "        )\n",
    "    model.add(hub_layer)\n",
    "    model.save('saved/')\n",
    "\n",
    "save_model(model_path)\n",
    "model = tf.keras.models.load_model('/content/saved')\n",
    "model.summary() # OK \n",
    "model.get_weights() # OK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = reconstructed_model.fit(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = load_model(model_file, compile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the input for the first period forecast\n",
    "dataset = df_lags.drop(columns=['T-1'])\n",
    "x_input = dataset.tail(1).values\n",
    "x_input = scaler.transform(x_input.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walk-forward forecast - one-step window - for the next 36 months \n",
    "for i in range(0, 36):\n",
    "    x = x_input[-60:].reshape((1, 60, 1))\n",
    "    output = model.predict(x)[0][0]\n",
    "    x_input = np.append(x_input, [output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse the scale\n",
    "input = scaler.inverse_transform(x_input.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidate the data\n",
    "df_pred = pd.DataFrame(\n",
    "    input[-36:],\n",
    "    columns=['IPCA'],\n",
    "    index=pd.date_range('2022-09-01', periods=36, freq='M')\n",
    "    )\n",
    "\n",
    "forecast = pd.concat([df, df_pred], ignore_index=False)\n",
    "\n",
    "forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The curves\n",
    "trace_2_1 = go.Scatter(\n",
    "    x=df[df.index>='2018-01-01'].index,\n",
    "    y=df.IPCA.values,\n",
    "    name='Actual',\n",
    "    marker=dict(color='SteelBlue')\n",
    "    )\n",
    "\n",
    "trace_2_2 = go.Scatter(\n",
    "    x=df_pred.index,\n",
    "    y=df_pred.IPCA.values,\n",
    "    name='Forecast',\n",
    "    marker=dict(color='Firebrick')\n",
    "    )\n",
    "\n",
    "fig_2 = make_subplots(specs=[[{\"secondary_y\": False}]])\n",
    "fig_2.add_trace(trace_2_1)\n",
    "fig_2.add_trace(trace_2_2,secondary_y=False)\n",
    "fig_2['layout'].update(\n",
    "    height=600,\n",
    "    width=800,\n",
    "    title='Actual vs Forecast',\n",
    "    xaxis=dict(tickangle=-90)\n",
    "    )\n",
    "\n",
    "iplot(fig_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using interpolation to convert the monthly data into daily samples\n",
    "daily_forecast = forecast.resample('D').interpolate()\n",
    "daily_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily Inflation Curve\n",
    "trace_3_1 = go.Scatter(\n",
    "    x=daily_forecast[(daily_forecast.index>'2010-01-01')\n",
    "                    & (daily_forecast.index<'2022-09-01')].index,\n",
    "    y=daily_forecast[(daily_forecast.index>'2010-01-01')\n",
    "                    & (daily_forecast.index<'2022-09-01')].IPCA.values,\n",
    "    name='Actual',\n",
    "    marker=dict(color='SteelBlue'),\n",
    "                    )\n",
    "\n",
    "trace_3_2 = go.Scatter(\n",
    "    x=daily_forecast[daily_forecast.index>='2022-09-01'].index,\n",
    "    y=daily_forecast[daily_forecast.index>='2022-09-01'].IPCA.values,\n",
    "    name='Forecast',\n",
    "    marker=dict(color='Firebrick'),\n",
    "    )\n",
    "\n",
    "fig_3 = make_subplots(specs=[[{\"secondary_y\": False}]])\n",
    "fig_3.add_trace(trace_3_1)\n",
    "fig_3.add_trace(trace_3_2,secondary_y=False)\n",
    "fig_3.update(layout=layout,)\n",
    "fig_3['layout'].update(\n",
    "    height=800,\n",
    "    width=1000,\n",
    "    title='Daily Inflation Curve',\n",
    "    xaxis=dict(tickangle=-90)\n",
    "    )\n",
    "\n",
    "iplot(fig_3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save the forecast as a Excel table to share and feed into other analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a table\n",
    "daily_forecast.to_excel(r'daily_inflation.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "Almosova, A., & Andresen, N. (2019). Nonlinear Inflation Forecasting with Recurrent Neural Networks.  \n",
    "Brownlee, J. (2017, April 7). Re: Time Series Forecasting with the Long Short-Term Memory Network in Python. Machine Learning Mastery. https://machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/  \n",
    "Chakraborty, C., & Joseph, A. (2017). Machine learning at central banks.  \n",
    "Garcia, M. G., Medeiros, M. C., & Vasconcelos, G. F. (2017). Real-time inflation forecasting with high-dimensional models: The case of Brazil. International Journal of Forecasting, 33(3), 679-693.  \n",
    "Lazzeri, F. (2020). Machine learning for time series forecasting with Python. John Wiley & Sons.  \n",
    "Medeiros, M. C., Vasconcelos, G. F., Veiga, Á., & Zilberman, E. (2021). Forecasting inflation in a data-rich environment: the benefits of machine learning methods. Journal of Business & Economic Statistics, 39(1), 98-119.  \n",
    "Paranhos, L. (2021). Predicting inflation with neural networks. arXiv preprint arXiv:2104.03757.  \n",
    "Schelter, B., Winterhalder, M., & Timmer, J. (2006). Handbook of time series analysis. Wiley-VCH, Berlin.  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a1de8a60ff369b5bad50827366df0a1872e3d8cae81ca60dd5fb58762ecc793"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
